{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90d5c7ad",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981fce41",
   "metadata": {},
   "source": [
    "## Fundamental Concept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77ff98e",
   "metadata": {},
   "source": [
    "- A neural network is a model that learns complex decision lines by defining a hidden layer between the input layer and the output layer.\n",
    "- It can be applied to both regression and classification, but it is mainly applied to classification problems.\n",
    "\n",
    "![img](simple_neural_network.png)\n",
    "\n",
    "- The above figure is a neural network in which the input layer is 2D, the hidden layer is 2D, and the output layer is 1D."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9135464a",
   "metadata": {},
   "source": [
    "![img](NIST_neural_network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cd7a7e",
   "metadata": {},
   "source": [
    "- After applying a neural network to classifying NIST, a dataset composed of cursive letters, we will check the classification results.\n",
    "- NIST has 10 cursive letters ranging from 0 to 9.\n",
    "- The input layer represents the input cursive numeric image itself.\n",
    "- The pixel values of each point are stored in a 1D column vector.\n",
    "- The hidden layer calculates a value received from the input layer with a nonlinear function such as a sigmoid function.\n",
    "- The larger the hyperparameters, the more complex decision lines are learned, but overfitting is likely to occur.\n",
    "- The output layer also calculates the value received from the hidden layer as a nonlinear function.\n",
    "- The output layer outputs 10 probabilities that the input cursive numerical image belongs to one of 0 to 9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae292ba",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9405f450",
   "metadata": {},
   "source": [
    "### fault perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23afc6a",
   "metadata": {},
   "source": [
    "- A fault perceptron is a model that identifies data by applying a nonlinear function to a result obtained by multiplying a feature by a weight.\n",
    "- For example, if feature is 2D, input feature is (x_1, x_2).\n",
    "![img](fault_perceptron.png)\n",
    "- we call w_1, w_2 as a weight, w_0 as a bias.\n",
    "- Weight and bias are parameters used for learning.\n",
    "- non-linear function f is called activation function\n",
    "- The probability y is calculated using the sum of the weights and features. In this case, the activation function uses a sigmoid function or the like.\n",
    "![img](activation_function.png)\n",
    "- It also simply shows only the input and output parts.\n",
    "![img](simple_activation_function.png)\n",
    "\n",
    "\n",
    "- Fault perceptrons have similar characteristics to logistic regression.\n",
    "- If activation function f is only sigmoid function, fault perceptron is same as logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a6114b",
   "metadata": {},
   "source": [
    "### neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356f7485",
   "metadata": {},
   "source": [
    "- A neural network is a model that defines several fault perceptrons to represent complex decision line.\n",
    "- fault perceptron can't learn decision line correctly.\n",
    "1. Set a hidden layer between the input and output.\n",
    "2. Set the decision to make the final decision by combining the output results of the two layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc52b22",
   "metadata": {},
   "source": [
    "## Sample Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "873fae76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9407407407407408"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = load_digits()\n",
    "\n",
    "X = data.images.reshape(len(data.images), -1)\n",
    "\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "model = MLPClassifier(hidden_layer_sizes=(16, ), max_iter = 1000)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057f33b4",
   "metadata": {},
   "source": [
    "## early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1951174",
   "metadata": {},
   "source": [
    "![img](early_stopping.png)\n",
    "- Some of the learning data is further divided into evaluation data used during learning.\n",
    "- It stores evaluation indicators such as loss in order with evaluation data used during learning so that the degree of learning progress can be known.\n",
    "- If there are signs of overfitting during learning, the learning is terminated in the middle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
